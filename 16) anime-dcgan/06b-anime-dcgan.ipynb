{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "06b-anime-dcgan.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zJWM0w8-7rPW",
        "JoOJIJ937rPY",
        "7HdpVR1n7rPa",
        "DHzgNaR57rPe",
        "Dt_tkA547rPf",
        "q_uc0l7r7rPh",
        "KaHPbcUr7rPl"
      ]
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.8 64-bit ('tano': conda)"
    },
    "interpreter": {
      "hash": "df1ebef89fe3587c862adfba157bc52a83d7313f5144167cfdbe9b8c1c95e131"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u900af4g9JiD"
      },
      "source": [
        "# Training Generative Adversarial Networks (GANs) in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h02mLy2t7rPM"
      },
      "source": [
        "# Introduction to Generative Modeling\n",
        "\n",
        "Deep neural networks은 주로 분류(classification)나 회귀(regression)와 같은 지도학습에 사용됩니다.\n",
        "하지만, GANs(Generative Adverserial Networks)은 신경망 생성 모델로써 매우 다른 목적으로 사용됩니다.\n",
        "\n",
        "> 생성 적 모델링은 입력 데이터의 규칙이나 패턴을 자동으로 발견하고 학습하는 머신 러닝의 비지도 학습 작업으로, 모델을 사용하여 원래 데이터 세트에서 그럴듯하게 가져올 수있는 새로운 예제를 생성하거나 출력 할 수 있습니다. - [더보기](https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/)\n",
        "\n",
        "\n",
        "생성 모델의 힘에 대해 알아 보려면 [thispersondoesnotexist.com](https://thispersondoesnotexist.com)을 방문하십시오. 페이지를 다시로드 할 때마다 사람 얼굴의 새로운 이미지가 즉석에서 생성됩니다. 결과는 매우 매력적입니다.\n",
        "\n",
        "\n",
        "<img src=\"https://imgix.bustle.com/inverse/4b/17/8f/0e/cf91/4506/99c7/e6a491c5d4ac/these-people-are-not-real--they-were-produced-by-our-generator-that-allows-control-over-different-a.png\" style=\"width:480px; margin-bottom:32px\"/>\n",
        "\n",
        "\n",
        "Generative Modeling에 사용되는 접근 방식이 많지만 본 수업에서의 Generative Adversarial Network는 다음 접근 방식을 사용해 보도록 하겠습니다.\n",
        "\n",
        "![GAN Flowchart](https://i.imgur.com/6NMdO9u.png)\n",
        "\n",
        "\n",
        "*Generator*와 *Discriminator*가 있습니다. Generator는 임의의 vector/matrix를 생성하면 \"가짜(fake)\" 샘플을 생성하고, Discriminator는 이를 \"진짜(real)\"(학습에 의한 데이터)인지 \"가짜(fake)\"(Generator에 의한 데이터)인지 분류(Classification)합니다.\n",
        "\n",
        "Generator와 Discriminator는 동시에 학습되는데, 몇 epoch동안 Discriminator를 먼저 학습시키고 나서 Generator를 학습하는 과정을 반복합니다. 이렇게하면 생성자와 판별자가 작업을 더 잘 수행할 수 있습니다.\n",
        "\n",
        "\n",
        "<img src=\"https://i.imgur.com/NaKtJs0.png\" width=\"360\" style=\"margin-bottom:32px\"/>\n",
        "\n",
        "\n",
        "\n",
        "GAN은, 학습이 매우 불안정(unstable)하며, 하이퍼 파라미터에 민감하고, 활성화 함수 및 정규화에 매우 민감합니다. \n",
        "\n",
        "본 예제에서는 63,000 개 이상의 잘린 애니메이션 얼굴로 구성된 [Anime Face Dataset]을 사용합니다. GAN은 비지도 학습 작업이므로 이미지에 레이블이 없습니다. 대부분의 코드는 [Reference] (https://www.kaggle.com/splcher/starter-anime-face-dataset)을 기반으로합니다. \n",
        "\n",
        "* 패키지 설치후 kaggle.json 파일을 이동\n",
        "* Account -> API -> \"Create New API Token\" -> C:\\User\\User Name\\.kaggle\\kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YNuH9MvaMuY"
      },
      "source": [
        "!pip install kaggle\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ffs2UlyBQhi"
      },
      "source": [
        "!kaggle datasets download -d splcher/animefacedataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiMLxHRL7rPO"
      },
      "source": [
        "The dataset has a single folder called `images` which contains all 63,000+ images in JPG format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2VLPRoq7rPP"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "fantasy_zip = zipfile.ZipFile('animefacedataset.zip')\n",
        "fantasy_zip.extractall('./animefacedataset')\n",
        " \n",
        "fantasy_zip.close()\n",
        "DATA_DIR = './animefacedataset'\n",
        "print(os.listdir(DATA_DIR))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hg9EwokF7rPQ"
      },
      "source": [
        "print(os.listdir(DATA_DIR+'/images')[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDRRG5907rPQ"
      },
      "source": [
        "`torchvision`의 `ImageFolder` 클래스를 사용하여 데이터 세트를 로드해 보겠습니다. 또한 이미지의 크기를 조정하고 64x64 픽셀로 자르고 각 채널에 대해 0.5의 평균 및 표준 편차로 픽셀 값을 정규화합니다.\n",
        " 이렇게하면 픽셀 값이 (-1, 1) 범위에 있게되어 Discriminator를 훈련하는 데 더 편리합니다. 또한 데이터를 일괄 적으로 로드하는 데이터 로더를 만들 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmRajdDe7rPR"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSlbIxS37rPR"
      },
      "source": [
        "image_size = 64\n",
        "batch_size = 128\n",
        "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLp6NN8B7rPR"
      },
      "source": [
        "train_ds = ImageFolder(DATA_DIR, transform=T.Compose([\n",
        "    T.Resize(image_size),\n",
        "    T.CenterCrop(image_size),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(*stats)]))\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n50e7mfZ7rPS"
      },
      "source": [
        "이미지 텐서를 비정규 화하고 훈련 배치의 샘플 이미지를 표시하는 도우미 함수를 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPVrSXO17rPS"
      },
      "source": [
        "import torch\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJTY6BMf7rPS"
      },
      "source": [
        "def denorm(img_tensors):\n",
        "    return img_tensors * stats[1][0] + stats[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jntkukw47rPT"
      },
      "source": [
        "def show_images(images, nmax=64):\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.set_xticks([]); ax.set_yticks([])\n",
        "    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
        "\n",
        "def show_batch(dl, nmax=64):\n",
        "    for images, _ in dl:\n",
        "        show_images(images, nmax)\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laQtLhl_7rPT"
      },
      "source": [
        "show_batch(train_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJWM0w8-7rPW"
      },
      "source": [
        "## GPU 사용\n",
        "\n",
        "GPU를 원활하게 사용하기 위해 사용 가능한 경우 몇 가지 도우미 함수 (`get_default_device` &`to_device`)와 도우미 클래스 `DeviceDataLoader`를 정의하여 모델 및 데이터를 GPU로 이동합니다 (사용 가능한 경우)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb8ORjt57rPX"
      },
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRxpHAp67rPX"
      },
      "source": [
        "이 노트북을 실행하는 위치에 따라 기본 장치는 CPU (torch.device ('cpu')) 또는 GPU (torch.device('cuda')) 일 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yutNeU5w7rPX"
      },
      "source": [
        "device = get_default_device()\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TOp0Zwt7rPY"
      },
      "source": [
        "이제 데이터 배치를 GPU로 자동 전송하기 위해`DeviceDataLoader`를 사용하여 훈련 데이터 로더를 이동할 수 있습니다 (사용 가능한 경우)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryTEXQuD7rPY"
      },
      "source": [
        "train_dl = DeviceDataLoader(train_dl, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoOJIJ937rPY"
      },
      "source": [
        "## Discriminator 모델\n",
        "\n",
        "Discriminator는 이미지를 입력으로 받아 \"real\"또는 \"generated\"으로 분류하려고합니다. CNN을 사용하여 구성해볼 것입니다. stride을 2로 설정하여 출력의 크기(Feature map)를 줄일 것입니다.\n",
        "\n",
        "\n",
        "![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/padding_strides_odd.gif)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htcCHpje7rPZ"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nq2pxPJ7rPZ"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "        # in: 3 x 64 x 64\n",
        "\n",
        "        nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        # out: 64 x 32 x 32\n",
        "\n",
        "        nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        # out: 128 x 16 x 16\n",
        "\n",
        "        nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        # out: 256 x 8 x 8\n",
        "\n",
        "        nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        # out: 512 x 4 x 4\n",
        "\n",
        "        nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "        # out: 1 x 1 x 1\n",
        "\n",
        "        nn.Flatten(),\n",
        "        nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "discriminator = Discriminator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzjyaUaC7rPZ"
      },
      "source": [
        "Discriminator에 대해 Leaky ReLU 활성화 함수(activate function)를 사용합니다.\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/1*ypsvQH7kvtI2BhzR2eT_Sw.png\" width=\"420\">\n",
        "\n",
        "\n",
        ">  일반 ReLU 함수와 달리 Leaky ReLU는 음수 값에 대해 작은 기울기 신호를 전달할 수 있습니다. 결과적으로 Discriminator의 기울기가 Generator로 더 강하게 흐릅니다. 역전파(back-prop) 시 0의 기울기를 전달하는 대신 작은 음(negative)의 기울기를 전달합니다.  - [Source](https://sthalles.github.io/advanced_gans/)\n",
        "\n",
        "\n",
        "다른 이진 분류(binary classification) 모델과 마찬가지로 Discriminator의 출력은 0과 1 사이의 단일 숫자이며 입력 이미지가 가짜(fake) 즉 생성될 가능성으로 해석(분류) 될 수 있습니다.\n",
        "\n",
        "분류 모델을 선택한 장치로 이동해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFT_FGnN7rPZ"
      },
      "source": [
        "discriminator = to_device(discriminator, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HdpVR1n7rPa"
      },
      "source": [
        "## Generator 모델\n",
        "\n",
        "Generator에 대한 입력은 일반적으로 이미지 생성을 위한 시드로 사용되는 벡터 또는 난수 행렬(Latent Vector or Tensor라고도 함)입니다. Generator는 `(128, 1, 1)` 크기의 잠재 Tensor를 `3 x 28 x 28`크기의 이미지 Tensor로 변환합니다. 이러한 크기를 변형하기 위하여 Pytorch의 `ConvTranspose2d` 레이어를 사용합니다. 해당 레이어는 *Transpose Convolution*(also referred *deconvolution*이라고도 함. 엄밀히는 아님!)으로 수행됩니다. [더 알아보기](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md#transposed-convolution-animations)\n",
        "\n",
        "![](https://i.imgur.com/DRvK546.gif)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "496i2VzW7rPa"
      },
      "source": [
        "latent_size = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuDQRN827rPc"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "        # in: latent_size x 1 x 1\n",
        "        nn.ConvTranspose2d(latent_size, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.ReLU(True),\n",
        "        # out: 512 x 4 x 4\n",
        "\n",
        "        nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(True),\n",
        "        # out: 256 x 8 x 8\n",
        "\n",
        "        nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(True),\n",
        "        # out: 128 x 16 x 16\n",
        "\n",
        "        nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(True),\n",
        "        # out: 64 x 32 x 32\n",
        "\n",
        "        nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.Tanh())\n",
        "        # out: 3 x 64 x 64\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "generator = Generator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T4IiCB97rPc"
      },
      "source": [
        "Generator의 레이어의 출력에 TanH 활성화 함수를 사용하였습니다.\n",
        "\n",
        "<img src=\"https://nic.schraudolph.org/teach/NNcourse/figs/tanh.gif\" width=\"420\" >\n",
        "\n",
        "> \"The ReLU activation  is used in the generator with the exception of the output layer which uses the Tanh function. We observed that using a bounded activation allowed the model to learn more quickly to saturate and cover the color space of the training distribution. Within the discriminator we found the leaky rectified activation (Maas et al., 2013) (Xu et al., 2015) to work well, especially for higher resolution modeling.\" - [Source](https://stackoverflow.com/questions/41489907/generative-adversarial-networks-tanh)\n",
        "\n",
        "\n",
        "TanH 활성화의 출력이 `[-1,1]` 범위에 있기 때문에 학습 데이터 세트의 이미지에 동일한 변환을 적용했습니다. Generator에서  출력 벡터를 생성하고 출력을 변환 및 비정규 화하여 이미지로 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_aHHWA07rPd"
      },
      "source": [
        "xb = torch.randn(batch_size, latent_size, 1, 1) # random latent tensors\n",
        "fake_images = generator(xb)\n",
        "print(fake_images.shape)\n",
        "show_images(fake_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6CwMocE7rPd"
      },
      "source": [
        "예상 할 수 있듯이 Generator의 출력은 기본적으로 랜덤 노이즈입니다. Generator의 출력 배치를 파일에 저장할 수있는 도우미 함수를 정의해 보겠습니다.\n",
        "\n",
        "Generator를 선택한 device로 이동합시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhsPkkCP7rPe"
      },
      "source": [
        "generator = to_device(generator, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHzgNaR57rPe"
      },
      "source": [
        "## Discriminator 학습\n",
        "\n",
        "Discriminator는 이진 분류 모델이므로 이진 교차 엔트로피 손실 함수(binary cross entropy loss function)를 사용하여 실제 이미지와 생성 된 이미지를 얼마나 잘 구별 할 수 있는지 정량화 할 수 있습니다.\n",
        "\n",
        "<img src=\"https://image.slidesharecdn.com/chrishokamp-dublinnlp3-160805110319/95/task-based-learning-for-nlp-going-beyond-cross-entropy-chris-hokamp-10-638.jpg?cb=1470395213\" width=\"420\" >"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz3jTbye7rPe"
      },
      "source": [
        "def train_discriminator(real_images, opt_d):\n",
        "    # Clear discriminator gradients\n",
        "    opt_d.zero_grad()\n",
        "\n",
        "    # Pass real images through discriminator\n",
        "    real_preds = discriminator(real_images)\n",
        "    real_targets = torch.ones(real_images.size(0), 1, device=device)\n",
        "    real_loss = F.binary_cross_entropy(real_preds, real_targets)\n",
        "    real_score = torch.mean(real_preds).item()\n",
        "    \n",
        "    # Generate fake images\n",
        "    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
        "    fake_images = generator(latent)\n",
        "\n",
        "    # Pass fake images through discriminator\n",
        "    fake_targets = torch.zeros(fake_images.size(0), 1, device=device)\n",
        "    fake_preds = discriminator(fake_images)\n",
        "    fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n",
        "    fake_score = torch.mean(fake_preds).item()\n",
        "\n",
        "    # Update discriminator weights\n",
        "    loss = real_loss + fake_loss\n",
        "    loss.backward()\n",
        "    opt_d.step()\n",
        "    return loss.item(), real_score, fake_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKyS1SKA7rPe"
      },
      "source": [
        "다음은 Discriminator를 훈련하는 단계입니다.\n",
        "\n",
        "\n",
        "- 이미지가 실제 Anime 데이터셋에서 선택되면 Discriminator가 1을 출력하고 0이면 생성된 것이라고 예상한다.\n",
        "\n",
        "- 먼저 실제 이미지 배치를 전달하고, loss를 계산한 다음에, 1로 설정한다.\n",
        "\n",
        "- 그 다음, Generator를 사용하여 가짜 이미지 배치를 생성하고 Discriminator에 전달하고 Loss를 계산하여 대상 레이블을 0으로 설정한다.\n",
        "\n",
        "- 마지막으로 두 손실을 더하고 전체 손실을 경사하강법을 사용하여 Discriminator에 weights를 조정한다.\n",
        "\n",
        "Discriminator를 훈련하는 동안에 Generator의 가중치를 변경하지 않는다는 것을 유의하는것이 중요함!!!!\n",
        "\n",
        "(`opt_d` 오직 `discriminator.parameters()`에만 영향을 미침!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt_tkA547rPf"
      },
      "source": [
        "## Generator 학습\n",
        "\n",
        "Generator의 출력은 이미지이기 때문에 Generator를 훈련할 수 있는 방법이 명확하지 않습니다.\n",
        "여기에서 Discriminator를 Loss Function의 일부로 사용하는 방법을 사용합니다. 작동 방식은 다음과 같습니다.\n",
        "\n",
        "\n",
        "- Generator를 사용하여 이미지 배치를 생성하고 Discriminator에 전달합니다.\n",
        "\n",
        "- 목표 라벨을 \"1(진짜, real)\"로 설정하여 loss를 계산합니다. Generator의 목적은 Discriminator를 속이는 것이 목적입니다.\n",
        "\n",
        "- Loss를 사용하여 경사하강법을 수행합니다. 즉, Generator의 가중치를 변경하므로 실제와 같은 이미지를 생성하는데 더 유용합니다.\n",
        "\n",
        "다음은 실제 코드 구현입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lmjNkZE7rPf"
      },
      "source": [
        "def train_generator(opt_g):\n",
        "    # Clear generator gradients\n",
        "    opt_g.zero_grad()\n",
        "    \n",
        "    # Generate fake images\n",
        "    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
        "    fake_images = generator(latent)\n",
        "    \n",
        "    # Try to fool the discriminator\n",
        "    preds = discriminator(fake_images)\n",
        "    targets = torch.ones(batch_size, 1, device=device)\n",
        "    loss = F.binary_cross_entropy(preds, targets)\n",
        "    \n",
        "    # Update generator weights\n",
        "    loss.backward()\n",
        "    opt_g.step()\n",
        "    \n",
        "    return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo00Pcmu7rPg"
      },
      "source": [
        "Generator의 특정 시드를 보여주고 저장할 수 있는 헬퍼 함수를 만들어보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiQyhcI37rPg"
      },
      "source": [
        "from torchvision.utils import save_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhMPV5bk7rPg"
      },
      "source": [
        "sample_dir = 'generated'\n",
        "os.makedirs(sample_dir, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiCEUuDE7rPg"
      },
      "source": [
        "def save_samples(index, latent_tensors, show=True):\n",
        "    fake_images = generator(latent_tensors)\n",
        "    fake_fname = 'generated-images-{0:0=4d}.png'.format(index)\n",
        "    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)\n",
        "    print('Saving', fake_fname)\n",
        "    if show:\n",
        "        fig, ax = plt.subplots(figsize=(8, 8))\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQNysyNY7rPg"
      },
      "source": [
        "생성기에 고정 된 입력 벡터 세트를 사용하여 모델을 학습 할 때 생성 된 개별 이미지가 시간이 지남에 따라 어떻게 진화하는지 확인합니다. 모델 학습을 시작하기 전에 한 세트의 이미지를 저장하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQN4Hvkm7rPg"
      },
      "source": [
        "fixed_latent = torch.randn(64, latent_size, 1, 1, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLZaqP0T7rPh"
      },
      "source": [
        "save_samples(0, fixed_latent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_uc0l7r7rPh"
      },
      "source": [
        "## 전체 학습 루프\n",
        "\n",
        "훈련 데이터의 각 배치에 대해 구분자와 생성기를 동시에 훈련하는 `fit`함수를 정의 해 보겠습니다.\n",
        " GAN에서 잘 작동하는 것으로 알려진 일부 사용자 지정 매개 변수 (betas)와 함께 Adam Optimizer를 사용할 것입니다. 또한 검사를 위해 샘플 생성 이미지를 정기적으로 저장합니다.\n",
        "\n",
        "\n",
        "<img src=\"https://i.imgur.com/6NMdO9u.png\" style=\"max-width:420px; margin-bottom:32px\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFfiZRa37rPh"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHsBi5hW7rPh"
      },
      "source": [
        "def fit(epochs, lr, start_idx=1):\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    # Losses & scores\n",
        "    losses_g = []\n",
        "    losses_d = []\n",
        "    real_scores = []\n",
        "    fake_scores = []\n",
        "    \n",
        "    # Create optimizers\n",
        "    opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        for real_images, _ in tqdm(train_dl):\n",
        "            # Train discriminator\n",
        "            loss_d, real_score, fake_score = train_discriminator(real_images, opt_d)\n",
        "            # Train generator\n",
        "            loss_g = train_generator(opt_g)\n",
        "            \n",
        "        # Record losses & scores\n",
        "        losses_g.append(loss_g)\n",
        "        losses_d.append(loss_d)\n",
        "        real_scores.append(real_score)\n",
        "        fake_scores.append(fake_score)\n",
        "        \n",
        "        # Log losses & scores (last batch)\n",
        "        print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n",
        "            epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n",
        "    \n",
        "        # Save generated images\n",
        "        save_samples(epoch+start_idx, fixed_latent, show=False)\n",
        "    \n",
        "    return losses_g, losses_d, real_scores, fake_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxw7tdof7rPh"
      },
      "source": [
        "이제 모델을 훈련 할 준비가되었습니다. 다른 학습률을 시도하여 Generator와 Discriminator 훈련 사이에 미세한 균형을 유지할 수 있는지 확인하십시오."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCO9Xz8a7rPi"
      },
      "source": [
        "lr = 0.0002\n",
        "epochs = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QukXeGTw7rPi"
      },
      "source": [
        "history = fit(epochs, lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzxo0l047rPi"
      },
      "source": [
        "losses_g, losses_d, real_scores, fake_scores = history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELs5OAbH7rPj"
      },
      "source": [
        "학습된 모델을 저장해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b55q3sQR7rPj"
      },
      "source": [
        "# Save the model checkpoints \n",
        "torch.save(generator.state_dict(), 'G.pth')\n",
        "torch.save(discriminator.state_dict(), 'D.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctJgUSgt7rPj"
      },
      "source": [
        "생성된 모델을 보고 첫번째, 5번째 마다 epoch결과를 보도록 합시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b033aUC07rPj"
      },
      "source": [
        "from IPython.display import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4Hpc2Wm7rPj"
      },
      "source": [
        "Image('./generated/generated-images-0001.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm4Yq2O67rPj"
      },
      "source": [
        "Image('./generated/generated-images-0005.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us-eJzle7rPk"
      },
      "source": [
        "Image('./generated/generated-images-0010.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9Jmym7z7rPk"
      },
      "source": [
        "Image('./generated/generated-images-0020.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9NI8e717rPk"
      },
      "source": [
        "Image('./generated/generated-images-0025.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp3Y77VU7rPk"
      },
      "source": [
        "OpenCV를 사용하여 각 Epoch 이후 생성 된 샘플 이미지를 비디오로 결합하여 훈련 과정을 시각화 할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfABlRRl7rPk"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "vid_fname = 'gans_training.avi'\n",
        "\n",
        "files = [os.path.join(sample_dir, f) for f in os.listdir(sample_dir) if 'generated' in f]\n",
        "files.sort()\n",
        "\n",
        "out = cv2.VideoWriter(vid_fname,cv2.VideoWriter_fourcc(*'MP4V'), 1, (530,530))\n",
        "[out.write(cv2.imread(fname)) for fname in files]\n",
        "out.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9OFv0oj7rPk"
      },
      "source": [
        "시간이 지남에 따라 손실이 어떻게 변하는지 시각화 할 수도 있습니다.\n",
        "손실을 시각화하는 것은 훈련 과정을 디버깅하는 데 매우 유용합니다.\n",
        "GAN의 경우 판별 자의 손실이 너무 높아지지 않고 시간이 지남에 따라 생성기의 손실이 줄어들 것으로 예상합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLdS3gb87rPk"
      },
      "source": [
        "plt.plot(losses_d, '-')\n",
        "plt.plot(losses_g, '-')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Discriminator', 'Generator'])\n",
        "plt.title('Losses');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McZ165r67rPl"
      },
      "source": [
        "plt.plot(real_scores, '-')\n",
        "plt.plot(fake_scores, '-')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('score')\n",
        "plt.legend(['Real', 'Fake'])\n",
        "plt.title('Scores');"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}